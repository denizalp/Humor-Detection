# -*- coding: utf-8 -*-
"""glove_baseline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d4AqF95pXvRhnGnQsPWXofiztoNXIL2Q

## Humor Detection
"""

from google.colab import drive
drive.mount('/gdrive', force_remount = True)

# required imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding,Flatten,Dense,Dropout,SimpleRNN,LSTM,Conv1D,MaxPooling1D,GlobalMaxPooling1D,GRU
from keras.optimizers import RMSprop
from keras import backend as K

import random

humor = open('/content/funny_glove.txt','r').read().split('\n')
not_humor = open('/content/notfunny_glove.txt','r').read().split('\n')

random.shuffle(humor)
random.shuffle(not_humor)

# take a sample
humor_ = humor
not_humor_ = not_humor

print("HUMOR:", len(humor_),"NOT HUMOR:", len(not_humor_))

import string

# preprocess data and remove various things...

def clean(review):
    return review.replace('\\n',' ')

for i in range(25000):
    humor_[i] = clean(humor_[i])
    not_humor_[i] = clean(not_humor_[i])



"""## Preprocessing"""

import numpy as np

texts = []
labels = []

np.random.shuffle(humor_)
np.random.shuffle(not_humor_)


for line in humor_:
    texts.append(line)
    labels.append(1)

for line in not_humor_:
    texts.append(line)
    labels.append(0)

print('Funny', len(humor_))    
print("Not Funny",len(not_humor_))  
print("Total", len(texts))

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

# Tokenizing words

# max word length
maxlen = 500
# take top 20k words only
max_words = 20000

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(texts)

sequences = tokenizer.texts_to_sequences(texts)
word_index = tokenizer.word_index

print('Found %s unique tokens'%len(word_index))

# padding the sequences
data = pad_sequences(sequences, maxlen=maxlen)

labels = np.array(labels)

print('Shape of data tensor:', data.shape)
print('Shape of labels tensor:', labels.shape)

# shuffle the data
indices = np.arange(data.shape[0])
np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]

"""## Loading the pre-trained word embeddings"""

# we would use stanford's GloVe pre-trained word embeddings
# download the GloVe word embeddings
!wget https://nlp.stanford.edu/data/glove.6B.zip

!unzip /content/glove.6B.zip.1



# parsing the GloVe word-embeddings file
embeddings_index = {}

f = open('glove.6B.300d.txt')
for line in f:
    values = line.split()
    word = values[0]
    coeffs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coeffs
f.close()

print(len(embeddings_index))



# preparing glove word embeddings matrix
embedding_dim = 300

embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i<max_words:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector # for words not in embedding index values will be zeros



"""## Building the model

### 1. Simple feed-forward network with dense layers on top of embedding layer --- using GloVe pre-trained word embeddings
"""

from keras.models import Sequential
from keras.layers import Embedding, Flatten, Dense, Dropout
from keras.optimizers import RMSprop


model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length=maxlen))
model.add(Flatten())

model.add(Dense(64, activation='relu'))
model.add(Dropout(0.3))

model.add(Dense(1, activation='sigmoid'))
model.summary()

"""### Load the pre-trained word embeddings into the embedding layer and freeze it"""

model.layers[0].set_weights([embedding_matrix])
model.layers[0].trainable = False

# train the model
model.compile(optimizer=RMSprop(lr=1e-6), loss='binary_crossentropy', metrics = ['acc'])
history = model.fit(data, labels, epochs=25, batch_size=32, validation_split=0.50)





# plotting the results

import matplotlib.pyplot as plt
# %matplotlib inline

def plot_result(history):
  acc = history.history['acc']
  val_acc = history.history['val_acc']
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  epochs = range(1, len(acc)+1)

  plt.plot(epochs, acc, label='Training acc')
  plt.plot(epochs, val_acc, label='Validation acc')
  plt.title('Training and validation accuracy')
  plt.legend()

  plt.figure()

  plt.plot(epochs, loss, label='Training loss')
  plt.plot(epochs, val_loss, label='Validation loss')
  plt.title('Training and validation loss')
  plt.legend()

  plt.show()

plot_result(history)

