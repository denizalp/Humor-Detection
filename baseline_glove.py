# -*- coding: utf-8 -*-
"""humor_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17i-L67k-UI4VNDEPA-ttTfh7vkJ2il-6

## Humor Detection
"""

from google.colab import drive
drive.mount('/gdrive', force_remount = True)

humour = open('funny2.txt','r').read().split('\n')
neg = open('not_funny2_trunc.txt','r').read().split('\n')

"""## Preprocessing"""

import numpy as np

texts = []
labels = []

np.random.shuffle(neg)


for line in humour:
    texts.append(line)
    labels.append(1)

for line in neg:
    texts.append(line)
    labels.append(0)

print('Funny', len(humour))    
print("Not Funny",len(neg))  
print("Total", len(texts))

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

# Tokenizing words

# max word length
maxlen = 50
# take top 10k words only
max_words = 10000

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(texts)

sequences = tokenizer.texts_to_sequences(texts)
word_index = tokenizer.word_index

print('Found %s unique tokens'%len(word_index))

# padding the sequences
data = pad_sequences(sequences, maxlen=maxlen)

labels = np.array(labels)

print('Shape of data tensor:', data.shape)
print('Shape of labels tensor:', labels.shape)

# shuffle the data
indices = np.arange(data.shape[0])
np.random.shuffle(indices)
data = data[indices]
labels = labels[indices]

"""## Loading the pre-trained word embeddings"""

# due to small size of dataset
# we would use standford's GloVe pre-trained word embeddings

# download the GloVe word embeddings
!wget http://nlp.stanford.edu/data/glove.6B.zip

!unzip glove.6B.zip

# parsing the GloVe word-embeddings file
embeddings_index = {}

f = open('glove.6B.100d.txt')
for line in f:
    values = line.split()
    word = values[0]
    coeffs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coeffs
f.close()

print(len(embeddings_index))

# preparing glove word embeddings matrix
embedding_dim = 100

embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i<max_words:
        embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector # for words not in embedding index values will be zeros

"""## Building the model

### 1. Simple feed-forward network with dense layers on top of embedding layer --- using GloVe pre-trained word embeddings
"""

from keras.models import Sequential
from keras.layers import Embedding, Flatten, Dense, Dropout
from keras.optimizers import RMSprop


model = Sequential()
model.add(Embedding(max_words, embedding_dim, input_length=maxlen))
model.add(Flatten())

model.add(Dense(64, activation='relu'))
model.add(Dropout(0.3))

model.add(Dense(1, activation='sigmoid'))
model.summary()

"""### Load the pre-trained word embeddings into the embedding layer and freeze it"""

model.layers[0].set_weights([embedding_matrix])
model.layers[0].trainable = False

# train the model
model.compile(optimizer=RMSprop(lr=1e-4), loss='binary_crossentropy', metrics = ['acc'])
history = model.fit(data, labels, epochs=20, batch_size=32, validation_split=0.1)

# plotting the results

import matplotlib.pyplot as plt
# %matplotlib inline

def plot_result(history):
  acc = history.history['acc']
  val_acc = history.history['val_acc']
  loss = history.history['loss']
  val_loss = history.history['val_loss']

  epochs = range(1, len(acc)+1)

  plt.plot(epochs, acc, label='Training acc')
  plt.plot(epochs, val_acc, label='Validation acc')
  plt.title('Training and validation accuracy')
  plt.legend()

  plt.figure()

  plt.plot(epochs, loss, label='Training loss')
  plt.plot(epochs, val_loss, label='Validation loss')
  plt.title('Training and validation loss')
  plt.legend()

  plt.show()

plot_result(history)

